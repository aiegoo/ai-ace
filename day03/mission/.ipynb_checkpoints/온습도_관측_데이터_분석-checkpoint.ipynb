{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bb1e736",
   "metadata": {},
   "source": [
    "# ğŸŒ¡ï¸ ì˜¨ìŠµë„ ê´€ì¸¡ ë°ì´í„° ë¶„ì„ (Temperature & Humidity Sensor Data Analysis)\n",
    "\n",
    "**WebEx ê°•ì˜ ì‹¤ìŠµ - Scikit-Learn Data Splitting ì¤‘ì‹¬**\n",
    "\n",
    "## ğŸ“‹ **í•™ìŠµ ëª©í‘œ:**\n",
    "1. **ì˜¨ìŠµë„ ì„¼ì„œ ë°ì´í„°** ë¡œë“œ ë° íƒìƒ‰\n",
    "2. **Scikit-Learn train_test_split** í™œìš©í•œ ë°ì´í„° ë¶„í• \n",
    "3. **Stratified Splitting** ìœ¼ë¡œ ë°ì´í„° ë¶„í¬ ìœ ì§€\n",
    "4. **Time Series Splitting** ìœ¼ë¡œ ì‹œê³„ì—´ ë°ì´í„° ì²˜ë¦¬\n",
    "5. **ë°ì´í„° ë¶„í•  í’ˆì§ˆ ê²€ì¦** ë° ëª¨ë¸ í•™ìŠµ\n",
    "\n",
    "## ğŸ¯ **í•µì‹¬ ê°œë…:**\n",
    "- **Train/Validation/Test Split** - ëª¨ë¸ í•™ìŠµ/ê²€ì¦/í‰ê°€ìš© ë°ì´í„° ë¶„ë¦¬\n",
    "- **Stratified Sampling** - í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€í•œ ë¶„í• \n",
    "- **Time Series Cross-Validation** - ì‹œê°„ ìˆœì„œ ê³ ë ¤í•œ ê²€ì¦\n",
    "- **Data Leakage ë°©ì§€** - ë¯¸ë˜ ì •ë³´ê°€ ê³¼ê±°ë¡œ ìœ ì¶œë˜ì§€ ì•Šë„ë¡ ë°©ì§€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c144b503",
   "metadata": {},
   "source": [
    "## ğŸ“š **Section 1: Import Required Libraries**\n",
    "í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ importí•˜ê³  í™˜ê²½ì„ ì„¤ì •í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75f2bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”§ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ Import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-Learn ë°ì´í„° ë¶„í•  ê´€ë ¨ ëª¨ë“ˆë“¤\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,      # ê¸°ë³¸ train/test ë¶„í• \n",
    "    StratifiedShuffleSplit, # ê³„ì¸µí™” ë¶„í• \n",
    "    TimeSeriesSplit,       # ì‹œê³„ì—´ ë¶„í• \n",
    "    cross_val_score,       # êµì°¨ê²€ì¦\n",
    "    validation_curve       # ê²€ì¦ ê³¡ì„ \n",
    ")\n",
    "\n",
    "# ëª¨ë¸ë§ ê´€ë ¨\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# í•œê¸€ í°íŠ¸ ì„¤ì • (matplotlib)\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"âœ… ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "print(f\"ğŸ“Š Pandas Version: {pd.__version__}\")\n",
    "print(f\"ğŸ”¢ NumPy Version: {np.__version__}\")\n",
    "print(f\"ğŸ¤– Scikit-Learn ë¶„í•  ëª¨ë“ˆë“¤ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "print(f\"ğŸ“ˆ Matplotlib & Seaborn ì‹œê°í™” ì¤€ë¹„ì™„ë£Œ!\")\n",
    "print(\"\\nğŸ¯ ì˜¨ìŠµë„ ì„¼ì„œ ë°ì´í„° ë¶„ì„ì„ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4b20e9",
   "metadata": {},
   "source": [
    "## ğŸ“Š **Section 2: Load Temperature and Humidity Data**\n",
    "ì˜¨ìŠµë„ ì„¼ì„œ ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³  ë¡œë“œí•˜ì—¬ êµ¬ì¡°ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebdb078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸŒ¡ï¸ ì˜¨ìŠµë„ ì„¼ì„œ ë°ì´í„° ìƒì„± (ì‹¤ì œ ì„¼ì„œ ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜)\n",
    "print(\"ğŸ”„ ì˜¨ìŠµë„ ì„¼ì„œ ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...\")\n",
    "\n",
    "# ì‹œê°„ ë²”ìœ„ ì„¤ì • (ìµœê·¼ 1ë…„ê°„ì˜ ì‹œê°„ë‹¹ ë°ì´í„°)\n",
    "np.random.seed(42)  # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ ì„¤ì •\n",
    "start_date = datetime(2024, 1, 1)\n",
    "end_date = datetime(2024, 12, 31)\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='H')\n",
    "\n",
    "# ì‹¤ì œì™€ ìœ ì‚¬í•œ ì˜¨ìŠµë„ íŒ¨í„´ ìƒì„±\n",
    "n_samples = len(date_range)\n",
    "\n",
    "# ê³„ì ˆë³„ ì˜¨ë„ íŒ¨í„´ (í•œêµ­ ê¸°í›„ ë°˜ì˜)\n",
    "day_of_year = date_range.dayofyear\n",
    "hour_of_day = date_range.hour\n",
    "\n",
    "# ê¸°ë³¸ ì˜¨ë„ íŒ¨í„´ (ê³„ì ˆì„± + ì¼ì¼ ì£¼ê¸°)\n",
    "base_temp = 15 + 10 * np.sin(2 * np.pi * day_of_year / 365.25)  # ê³„ì ˆ íŒ¨í„´\n",
    "daily_temp = 5 * np.sin(2 * np.pi * hour_of_day / 24)  # ì¼ì¼ íŒ¨í„´\n",
    "noise_temp = np.random.normal(0, 2, n_samples)  # ë…¸ì´ì¦ˆ\n",
    "temperature = base_temp + daily_temp + noise_temp\n",
    "\n",
    "# ìŠµë„ íŒ¨í„´ (ì˜¨ë„ì™€ ë°˜ë¹„ë¡€ ê´€ê³„ + ê³„ì ˆì„±)\n",
    "base_humidity = 60 + 20 * np.sin(2 * np.pi * (day_of_year + 90) / 365.25)  # ê³„ì ˆ íŒ¨í„´\n",
    "temp_humidity = -0.5 * (temperature - 20)  # ì˜¨ë„ì™€ ë°˜ë¹„ë¡€\n",
    "noise_humidity = np.random.normal(0, 5, n_samples)  # ë…¸ì´ì¦ˆ\n",
    "humidity = np.clip(base_humidity + temp_humidity + noise_humidity, 10, 95)\n",
    "\n",
    "# ê¸°ìƒ ì¡°ê±´ ë¶„ë¥˜ (ì˜¨ìŠµë„ ê¸°ë°˜)\n",
    "conditions = []\n",
    "for temp, hum in zip(temperature, humidity):\n",
    "    if temp < 5:\n",
    "        condition = \"ì¶”ìœ„\" if hum < 60 else \"ìŠµí•œì¶”ìœ„\"\n",
    "    elif temp < 20:\n",
    "        condition = \"ì„œëŠ˜í•¨\" if hum < 60 else \"ìŠµí•œì„œëŠ˜í•¨\"\n",
    "    elif temp < 30:\n",
    "        condition = \"ì ì •\" if hum < 70 else \"ìŠµí•¨\"\n",
    "    else:\n",
    "        condition = \"ë”ìœ„\" if hum < 80 else \"ë¬´ë”ìœ„\"\n",
    "    conditions.append(condition)\n",
    "\n",
    "# ì„¼ì„œ ìœ„ì¹˜ (ì‹¤ë‚´/ì‹¤ì™¸ êµ¬ë¶„)\n",
    "sensor_locations = np.random.choice(['ì‹¤ë‚´', 'ì‹¤ì™¸'], n_samples, p=[0.6, 0.4])\n",
    "\n",
    "# ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "sensor_data = pd.DataFrame({\n",
    "    'timestamp': date_range,\n",
    "    'temperature': np.round(temperature, 1),\n",
    "    'humidity': np.round(humidity, 1),\n",
    "    'condition': conditions,\n",
    "    'location': sensor_locations,\n",
    "    'day_of_week': date_range.day_name(),\n",
    "    'hour': date_range.hour,\n",
    "    'month': date_range.month,\n",
    "    'season': pd.cut(date_range.month, \n",
    "                     bins=[0, 3, 6, 9, 12], \n",
    "                     labels=['ê²¨ìš¸', 'ë´„', 'ì—¬ë¦„', 'ê°€ì„'])\n",
    "})\n",
    "\n",
    "print(f\"âœ… ì˜¨ìŠµë„ ì„¼ì„œ ë°ì´í„° ìƒì„± ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“ ë°ì´í„° í¬ê¸°: {sensor_data.shape}\")\n",
    "print(f\"ğŸ“… ê¸°ê°„: {sensor_data['timestamp'].min()} ~ {sensor_data['timestamp'].max()}\")\n",
    "print(f\"ğŸŒ¡ï¸  ì˜¨ë„ ë²”ìœ„: {sensor_data['temperature'].min():.1f}Â°C ~ {sensor_data['temperature'].max():.1f}Â°C\")\n",
    "print(f\"ğŸ’§ ìŠµë„ ë²”ìœ„: {sensor_data['humidity'].min():.1f}% ~ {sensor_data['humidity'].max():.1f}%\")\n",
    "\n",
    "# ë°ì´í„° êµ¬ì¡° í™•ì¸\n",
    "print(f\"\\nğŸ“Š ë°ì´í„° êµ¬ì¡°:\")\n",
    "print(sensor_data.info())\n",
    "\n",
    "print(f\"\\nğŸ“ˆ ì²« 10ê°œ ë ˆì½”ë“œ:\")\n",
    "display(sensor_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f7b06b",
   "metadata": {},
   "source": [
    "## ğŸ” **Section 3: Data Exploration and Preprocessing**\n",
    "ë°ì´í„° íŠ¹ì„±ì„ íƒìƒ‰í•˜ê³  ê²°ì¸¡ê°’ì„ ì²˜ë¦¬í•˜ë©° ë¶„í• ì„ ìœ„í•œ í”¼ì²˜ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b317b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” ë°ì´í„° íƒìƒ‰ì  ë¶„ì„ (EDA)\n",
    "print(\"ğŸ” ì˜¨ìŠµë„ ë°ì´í„° íƒìƒ‰ì  ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "\n",
    "# 1. ê¸°ìˆ í†µê³„ëŸ‰\n",
    "print(\"\\nğŸ“Š ê¸°ìˆ í†µê³„ëŸ‰:\")\n",
    "print(sensor_data[['temperature', 'humidity']].describe())\n",
    "\n",
    "# 2. ê²°ì¸¡ê°’ í™•ì¸\n",
    "print(f\"\\nâ“ ê²°ì¸¡ê°’ í™•ì¸:\")\n",
    "missing_values = sensor_data.isnull().sum()\n",
    "print(missing_values)\n",
    "\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"âœ… ê²°ì¸¡ê°’ì´ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "else:\n",
    "    print(f\"âš ï¸ ì´ {missing_values.sum()}ê°œì˜ ê²°ì¸¡ê°’ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# 3. ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„í¬\n",
    "print(f\"\\nğŸ“ˆ ê¸°ìƒ ì¡°ê±´ ë¶„í¬:\")\n",
    "condition_counts = sensor_data['condition'].value_counts()\n",
    "print(condition_counts)\n",
    "\n",
    "print(f\"\\nğŸ“ ì„¼ì„œ ìœ„ì¹˜ ë¶„í¬:\")\n",
    "location_counts = sensor_data['location'].value_counts()\n",
    "print(location_counts)\n",
    "\n",
    "# 4. ì‹œê°í™”\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# ì˜¨ë„ ë¶„í¬\n",
    "axes[0, 0].hist(sensor_data['temperature'], bins=50, alpha=0.7, color='red')\n",
    "axes[0, 0].set_title('ì˜¨ë„ ë¶„í¬')\n",
    "axes[0, 0].set_xlabel('ì˜¨ë„ (Â°C)')\n",
    "axes[0, 0].set_ylabel('ë¹ˆë„')\n",
    "\n",
    "# ìŠµë„ ë¶„í¬\n",
    "axes[0, 1].hist(sensor_data['humidity'], bins=50, alpha=0.7, color='blue')\n",
    "axes[0, 1].set_title('ìŠµë„ ë¶„í¬')\n",
    "axes[0, 1].set_xlabel('ìŠµë„ (%)')\n",
    "axes[0, 1].set_ylabel('ë¹ˆë„')\n",
    "\n",
    "# ì˜¨ë„-ìŠµë„ ê´€ê³„\n",
    "scatter = axes[1, 0].scatter(sensor_data['temperature'], sensor_data['humidity'], \n",
    "                             alpha=0.5, c=sensor_data['month'], cmap='viridis')\n",
    "axes[1, 0].set_title('ì˜¨ë„-ìŠµë„ ê´€ê³„ (ì›”ë³„ ìƒ‰ìƒ)')\n",
    "axes[1, 0].set_xlabel('ì˜¨ë„ (Â°C)')\n",
    "axes[1, 0].set_ylabel('ìŠµë„ (%)')\n",
    "plt.colorbar(scatter, ax=axes[1, 0], label='ì›”')\n",
    "\n",
    "# ê¸°ìƒ ì¡°ê±´ë³„ ë¶„í¬\n",
    "condition_counts.plot(kind='bar', ax=axes[1, 1], color='green', alpha=0.7)\n",
    "axes[1, 1].set_title('ê¸°ìƒ ì¡°ê±´ë³„ ë¶„í¬')\n",
    "axes[1, 1].set_xlabel('ê¸°ìƒ ì¡°ê±´')\n",
    "axes[1, 1].set_ylabel('ë¹ˆë„')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… ë°ì´í„° íƒìƒ‰ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fd5776",
   "metadata": {},
   "source": [
    "## ğŸ”ª **Section 4: Train-Test Split with Scikit-Learn**\n",
    "**WebEx ê°•ì˜ í•µì‹¬**: Scikit-Learnì˜ train_test_splitì„ í™œìš©í•œ ê¸°ë³¸ì ì¸ ë°ì´í„° ë¶„í• "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ee32fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”ª Scikit-Learn train_test_split í™œìš©\n",
    "print(\"ğŸ”ª SCIKIT-LEARN TRAIN_TEST_SPLIT ì‹¤ìŠµ\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# í”¼ì²˜ì™€ íƒ€ê²Ÿ ë³€ìˆ˜ ì¤€ë¹„\n",
    "print(\"\\n1ï¸âƒ£ í”¼ì²˜ì™€ íƒ€ê²Ÿ ë³€ìˆ˜ ì„¤ì •:\")\n",
    "\n",
    "# ì˜ˆì œ 1: íšŒê·€ ë¬¸ì œ (ì˜¨ë„ ì˜ˆì¸¡)\n",
    "# í”¼ì²˜: ìŠµë„, ì‹œê°„, ì›”, ì„¼ì„œìœ„ì¹˜\n",
    "feature_columns = ['humidity', 'hour', 'month']\n",
    "\n",
    "# ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©\n",
    "le_location = LabelEncoder()\n",
    "sensor_data['location_encoded'] = le_location.fit_transform(sensor_data['location'])\n",
    "\n",
    "# í”¼ì²˜ í–‰ë ¬ X êµ¬ì„±\n",
    "X_regression = sensor_data[feature_columns + ['location_encoded']].copy()\n",
    "y_regression = sensor_data['temperature'].copy()  # ì˜¨ë„ ì˜ˆì¸¡\n",
    "\n",
    "print(f\"ğŸ“Š íšŒê·€ ë¬¸ì œ ì„¤ì •:\")\n",
    "print(f\"   â€¢ í”¼ì²˜ (X): {X_regression.columns.tolist()}\")\n",
    "print(f\"   â€¢ íƒ€ê²Ÿ (y): ì˜¨ë„ ì˜ˆì¸¡\")\n",
    "print(f\"   â€¢ ë°ì´í„° í¬ê¸°: {X_regression.shape}\")\n",
    "\n",
    "# ì˜ˆì œ 2: ë¶„ë¥˜ ë¬¸ì œ (ê¸°ìƒ ì¡°ê±´ ì˜ˆì¸¡)\n",
    "X_classification = X_regression.copy()\n",
    "y_classification = sensor_data['condition'].copy()  # ê¸°ìƒ ì¡°ê±´ ë¶„ë¥˜\n",
    "\n",
    "print(f\"\\nğŸ“Š ë¶„ë¥˜ ë¬¸ì œ ì„¤ì •:\")\n",
    "print(f\"   â€¢ í”¼ì²˜ (X): {X_classification.columns.tolist()}\")\n",
    "print(f\"   â€¢ íƒ€ê²Ÿ (y): ê¸°ìƒ ì¡°ê±´ ë¶„ë¥˜\")\n",
    "print(f\"   â€¢ í´ë˜ìŠ¤ ìˆ˜: {y_classification.nunique()}ê°œ\")\n",
    "print(f\"   â€¢ í´ë˜ìŠ¤: {y_classification.unique()}\")\n",
    "\n",
    "print(f\"\\n2ï¸âƒ£ ê¸°ë³¸ Train-Test Split (70:30 ë¹„ìœ¨):\")\n",
    "\n",
    "# íšŒê·€ ë¬¸ì œ ë¶„í• \n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_regression, y_regression, \n",
    "    test_size=0.3,           # 30%ë¥¼ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ\n",
    "    random_state=42,         # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼\n",
    "    shuffle=True             # ë°ì´í„° ì…”í”Œ\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“ˆ íšŒê·€ ë¬¸ì œ ë¶„í•  ê²°ê³¼:\")\n",
    "print(f\"   â€¢ í›ˆë ¨ ì„¸íŠ¸: {X_train_reg.shape[0]:,}ê°œ ({X_train_reg.shape[0]/len(X_regression)*100:.1f}%)\")\n",
    "print(f\"   â€¢ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: {X_test_reg.shape[0]:,}ê°œ ({X_test_reg.shape[0]/len(X_regression)*100:.1f}%)\")\n",
    "print(f\"   â€¢ í›ˆë ¨ ì˜¨ë„ ë²”ìœ„: {y_train_reg.min():.1f}Â°C ~ {y_train_reg.max():.1f}Â°C\")\n",
    "print(f\"   â€¢ í…ŒìŠ¤íŠ¸ ì˜¨ë„ ë²”ìœ„: {y_test_reg.min():.1f}Â°C ~ {y_test_reg.max():.1f}Â°C\")\n",
    "\n",
    "# ë¶„ë¥˜ ë¬¸ì œ ë¶„í• \n",
    "X_train_clf, X_test_clf, y_train_clf, y_test_clf = train_test_split(\n",
    "    X_classification, y_classification,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“Š ë¶„ë¥˜ ë¬¸ì œ ë¶„í•  ê²°ê³¼:\")\n",
    "print(f\"   â€¢ í›ˆë ¨ ì„¸íŠ¸: {X_train_clf.shape[0]:,}ê°œ\")\n",
    "print(f\"   â€¢ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: {X_test_clf.shape[0]:,}ê°œ\")\n",
    "\n",
    "# í›ˆë ¨/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸\n",
    "train_dist = y_train_clf.value_counts(normalize=True).sort_index()\n",
    "test_dist = y_test_clf.value_counts(normalize=True).sort_index()\n",
    "\n",
    "print(f\"\\nğŸ“ˆ í´ë˜ìŠ¤ ë¶„í¬ ë¹„êµ:\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'í›ˆë ¨ì„¸íŠ¸': train_dist,\n",
    "    'í…ŒìŠ¤íŠ¸ì„¸íŠ¸': test_dist\n",
    "})\n",
    "print(comparison_df.round(3))\n",
    "\n",
    "print(f\"\\n3ï¸âƒ£ ë‹¤ì–‘í•œ ë¶„í•  ë¹„ìœ¨ ì‹¤í—˜:\")\n",
    "\n",
    "# ë‹¤ì–‘í•œ test_sizeë¡œ ì‹¤í—˜\n",
    "test_sizes = [0.2, 0.25, 0.3, 0.4]\n",
    "split_results = []\n",
    "\n",
    "for test_size in test_sizes:\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "        X_regression, y_regression,\n",
    "        test_size=test_size,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    split_results.append({\n",
    "        'test_size': f\"{test_size*100:.0f}%\",\n",
    "        'train_samples': len(X_tr),\n",
    "        'test_samples': len(X_te),\n",
    "        'train_ratio': f\"{(1-test_size)*100:.0f}%\",\n",
    "        'test_ratio': f\"{test_size*100:.0f}%\"\n",
    "    })\n",
    "\n",
    "split_df = pd.DataFrame(split_results)\n",
    "print(\"ë‹¤ì–‘í•œ ë¶„í•  ë¹„ìœ¨ ê²°ê³¼:\")\n",
    "print(split_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nâœ… Train-Test Split ì™„ë£Œ!\")\n",
    "print(f\"ğŸ¯ ë‹¤ìŒ ë‹¨ê³„: Stratified Splitìœ¼ë¡œ í´ë˜ìŠ¤ ë¶„í¬ ìœ ì§€í•˜ê¸°\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e21b84",
   "metadata": {},
   "source": [
    "## ğŸ¯ **Section 5: Stratified Splitting for Sensor Data**\n",
    "ê³„ì¸µí™” ë¶„í• ë¡œ í´ë˜ìŠ¤ ë¶„í¬ë¥¼ ìœ ì§€í•˜ë©´ì„œ ë°ì´í„°ë¥¼ ë‚˜ëˆ„ëŠ” ê³ ê¸‰ ê¸°ë²•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64160145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ Stratified Splitting - í´ë˜ìŠ¤ ë¶„í¬ ìœ ì§€\n",
    "print(\"ğŸ¯ STRATIFIED SPLITTING ì‹¤ìŠµ\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n1ï¸âƒ£ ì¼ë°˜ ë¶„í•  vs ê³„ì¸µí™” ë¶„í•  ë¹„êµ:\")\n",
    "\n",
    "# ì¼ë°˜ ë¶„í• \n",
    "X_train_normal, X_test_normal, y_train_normal, y_test_normal = train_test_split(\n",
    "    X_classification, y_classification,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# ê³„ì¸µí™” ë¶„í•  (stratify ë§¤ê°œë³€ìˆ˜ ì‚¬ìš©)\n",
    "X_train_strat, X_test_strat, y_train_strat, y_test_strat = train_test_split(\n",
    "    X_classification, y_classification,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    shuffle=True,\n",
    "    stratify=y_classification  # ğŸ”‘ í•µì‹¬: í´ë˜ìŠ¤ ë¶„í¬ ìœ ì§€\n",
    ")\n",
    "\n",
    "print(\"ğŸ“Š í´ë˜ìŠ¤ ë¶„í¬ ë¹„êµ:\")\n",
    "\n",
    "# ì›ë³¸ ë°ì´í„° ë¶„í¬\n",
    "original_dist = y_classification.value_counts(normalize=True).sort_index()\n",
    "\n",
    "# ì¼ë°˜ ë¶„í•  ê²°ê³¼\n",
    "normal_train_dist = y_train_normal.value_counts(normalize=True).sort_index()\n",
    "normal_test_dist = y_test_normal.value_counts(normalize=True).sort_index()\n",
    "\n",
    "# ê³„ì¸µí™” ë¶„í•  ê²°ê³¼\n",
    "strat_train_dist = y_train_strat.value_counts(normalize=True).sort_index()\n",
    "strat_test_dist = y_test_strat.value_counts(normalize=True).sort_index()\n",
    "\n",
    "# ê²°ê³¼ ì •ë¦¬\n",
    "comparison_df = pd.DataFrame({\n",
    "    'ì›ë³¸ë°ì´í„°': original_dist,\n",
    "    'ì¼ë°˜ë¶„í• _í›ˆë ¨': normal_train_dist,\n",
    "    'ì¼ë°˜ë¶„í• _í…ŒìŠ¤íŠ¸': normal_test_dist,\n",
    "    'ê³„ì¸µë¶„í• _í›ˆë ¨': strat_train_dist,\n",
    "    'ê³„ì¸µë¶„í• _í…ŒìŠ¤íŠ¸': strat_test_dist\n",
    "})\n",
    "\n",
    "print(comparison_df.round(3))\n",
    "\n",
    "# ë¶„í¬ ì°¨ì´ ê³„ì‚°\n",
    "print(f\"\\nğŸ“ˆ ì›ë³¸ ë°ì´í„°ì™€ì˜ ë¶„í¬ ì°¨ì´ (ì ˆëŒ“ê°’ í‰ê· ):\")\n",
    "normal_train_diff = np.mean(np.abs(normal_train_dist - original_dist))\n",
    "normal_test_diff = np.mean(np.abs(normal_test_dist - original_dist))\n",
    "strat_train_diff = np.mean(np.abs(strat_train_dist - original_dist))\n",
    "strat_test_diff = np.mean(np.abs(strat_test_dist - original_dist))\n",
    "\n",
    "print(f\"   â€¢ ì¼ë°˜ ë¶„í•  - í›ˆë ¨ì„¸íŠ¸: {normal_train_diff:.4f}\")\n",
    "print(f\"   â€¢ ì¼ë°˜ ë¶„í•  - í…ŒìŠ¤íŠ¸ì„¸íŠ¸: {normal_test_diff:.4f}\")\n",
    "print(f\"   â€¢ ê³„ì¸µ ë¶„í•  - í›ˆë ¨ì„¸íŠ¸: {strat_train_diff:.4f}\")\n",
    "print(f\"   â€¢ ê³„ì¸µ ë¶„í•  - í…ŒìŠ¤íŠ¸ì„¸íŠ¸: {strat_test_diff:.4f}\")\n",
    "\n",
    "print(f\"\\n2ï¸âƒ£ StratifiedShuffleSplit í™œìš©:\")\n",
    "\n",
    "# StratifiedShuffleSplit ì‚¬ìš©\n",
    "sss = StratifiedShuffleSplit(\n",
    "    n_splits=5,        # 5ë²ˆ ë¶„í• \n",
    "    test_size=0.3,     # 30% í…ŒìŠ¤íŠ¸\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"ğŸ“Š ì—¬ëŸ¬ ë²ˆì˜ ê³„ì¸µí™” ë¶„í•  ê²°ê³¼:\")\n",
    "\n",
    "split_results = []\n",
    "for i, (train_idx, test_idx) in enumerate(sss.split(X_classification, y_classification)):\n",
    "    y_train_split = y_classification.iloc[train_idx]\n",
    "    y_test_split = y_classification.iloc[test_idx]\n",
    "    \n",
    "    train_dist = y_train_split.value_counts(normalize=True).sort_index()\n",
    "    test_dist = y_test_split.value_counts(normalize=True).sort_index()\n",
    "    \n",
    "    # ì›ë³¸ê³¼ì˜ ì°¨ì´ ê³„ì‚°\n",
    "    train_diff = np.mean(np.abs(train_dist - original_dist))\n",
    "    test_diff = np.mean(np.abs(test_dist - original_dist))\n",
    "    \n",
    "    split_results.append({\n",
    "        'Split': f\"ë¶„í• _{i+1}\",\n",
    "        'í›ˆë ¨ì„¸íŠ¸_ì°¨ì´': train_diff,\n",
    "        'í…ŒìŠ¤íŠ¸ì„¸íŠ¸_ì°¨ì´': test_diff,\n",
    "        'ì „ì²´_ì°¨ì´': (train_diff + test_diff) / 2\n",
    "    })\n",
    "\n",
    "split_df = pd.DataFrame(split_results)\n",
    "print(split_df.round(4))\n",
    "\n",
    "print(f\"\\n3ï¸âƒ£ ê³„ì¸µí™” ë¶„í• ì˜ ì‹œê°í™”:\")\n",
    "\n",
    "# ì‹œê°í™”\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# ì›ë³¸ ë°ì´í„° ë¶„í¬\n",
    "original_dist.plot(kind='bar', ax=axes[0], title='ì›ë³¸ ë°ì´í„° ë¶„í¬', color='gray')\n",
    "axes[0].set_ylabel('ë¹„ìœ¨')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# ì¼ë°˜ ë¶„í•  ê²°ê³¼\n",
    "normal_comparison = pd.DataFrame({\n",
    "    'í›ˆë ¨ì„¸íŠ¸': normal_train_dist,\n",
    "    'í…ŒìŠ¤íŠ¸ì„¸íŠ¸': normal_test_dist\n",
    "})\n",
    "normal_comparison.plot(kind='bar', ax=axes[1], title='ì¼ë°˜ ë¶„í•  ê²°ê³¼')\n",
    "axes[1].set_ylabel('ë¹„ìœ¨')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# ê³„ì¸µí™” ë¶„í•  ê²°ê³¼\n",
    "strat_comparison = pd.DataFrame({\n",
    "    'í›ˆë ¨ì„¸íŠ¸': strat_train_dist,\n",
    "    'í…ŒìŠ¤íŠ¸ì„¸íŠ¸': strat_test_dist\n",
    "})\n",
    "strat_comparison.plot(kind='bar', ax=axes[2], title='ê³„ì¸µí™” ë¶„í•  ê²°ê³¼')\n",
    "axes[2].set_ylabel('ë¹„ìœ¨')\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ… Stratified Splitting ì™„ë£Œ!\")\n",
    "print(f\"ğŸ¯ í•µì‹¬: ê³„ì¸µí™” ë¶„í• ì´ í´ë˜ìŠ¤ ë¶„í¬ë¥¼ ë” ì˜ ìœ ì§€í•©ë‹ˆë‹¤!\")\n",
    "print(f\"ğŸ“š WebEx ê°•ì˜ í¬ì¸íŠ¸: stratify ë§¤ê°œë³€ìˆ˜ ì‚¬ìš©ë²• ìˆ™ì§€!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae310c4e",
   "metadata": {},
   "source": [
    "## â° **Section 6: Time Series Splitting for Sequential Data**\n",
    "ì‹œê³„ì—´ ë°ì´í„°ì—ì„œ ì‹œê°„ ìˆœì„œë¥¼ ê³ ë ¤í•œ ë¶„í•  ê¸°ë²• (ë§¤ìš° ì¤‘ìš”!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912a6230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â° Time Series Splitting - ì‹œê°„ ìˆœì„œ ê³ ë ¤\n",
    "print(\"â° TIME SERIES SPLITTING ì‹¤ìŠµ\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\nğŸš¨ ì¤‘ìš”: ì‹œê³„ì—´ ë°ì´í„°ì—ì„œëŠ” ë¯¸ë˜ ì •ë³´ê°€ ê³¼ê±°ë¡œ ìœ ì¶œë˜ë©´ ì•ˆë©ë‹ˆë‹¤!\")\n",
    "print(\"   â†’ Data Leakage ë°©ì§€ë¥¼ ìœ„í•´ ì‹œê°„ ìˆœì„œë¥¼ ë°˜ë“œì‹œ ìœ ì§€í•´ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "print(\"\\n1ï¸âƒ£ ì‹œê°„ ìˆœì„œ ê¸°ë°˜ ë¶„í• :\")\n",
    "\n",
    "# ë°ì´í„°ë¥¼ ì‹œê°„ ìˆœì„œëŒ€ë¡œ ì •ë ¬\n",
    "sensor_data_sorted = sensor_data.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "# ì‹œê°„ ê¸°ë°˜ ë¶„í•  (ì²˜ìŒ 70%ëŠ” í›ˆë ¨, ë‚˜ë¨¸ì§€ 30%ëŠ” í…ŒìŠ¤íŠ¸)\n",
    "split_point = int(len(sensor_data_sorted) * 0.7)\n",
    "\n",
    "train_data = sensor_data_sorted[:split_point]\n",
    "test_data = sensor_data_sorted[split_point:]\n",
    "\n",
    "print(f\"ğŸ“… ì‹œê°„ ê¸°ë°˜ ë¶„í•  ê²°ê³¼:\")\n",
    "print(f\"   â€¢ í›ˆë ¨ ê¸°ê°„: {train_data['timestamp'].min()} ~ {train_data['timestamp'].max()}\")\n",
    "print(f\"   â€¢ í…ŒìŠ¤íŠ¸ ê¸°ê°„: {test_data['timestamp'].min()} ~ {test_data['timestamp'].max()}\")\n",
    "print(f\"   â€¢ í›ˆë ¨ ë°ì´í„°: {len(train_data):,}ê°œ ({len(train_data)/len(sensor_data_sorted)*100:.1f}%)\")\n",
    "print(f\"   â€¢ í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_data):,}ê°œ ({len(test_data)/len(sensor_data_sorted)*100:.1f}%)\")\n",
    "\n",
    "# íŠ¹ì§•ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬\n",
    "X_train_ts = train_data[['humidity', 'hour', 'month', 'location_encoded']]\n",
    "y_train_ts = train_data['temperature']\n",
    "X_test_ts = test_data[['humidity', 'hour', 'month', 'location_encoded']]\n",
    "y_test_ts = test_data['temperature']\n",
    "\n",
    "print(f\"\\n2ï¸âƒ£ TimeSeriesSplitì„ í™œìš©í•œ êµì°¨ê²€ì¦:\")\n",
    "\n",
    "# TimeSeriesSplit ì„¤ì •\n",
    "tscv = TimeSeriesSplit(n_splits=5)  # 5-fold ì‹œê³„ì—´ êµì°¨ê²€ì¦\n",
    "\n",
    "print(\"ğŸ“Š TimeSeriesSplit ë¶„í•  ì‹œê°í™”:\")\n",
    "\n",
    "# ë¶„í•  ì •ë³´ ì €ì¥\n",
    "split_info = []\n",
    "for i, (train_idx, test_idx) in enumerate(tscv.split(X_train_ts)):\n",
    "    train_start = train_data.iloc[train_idx[0]]['timestamp']\n",
    "    train_end = train_data.iloc[train_idx[-1]]['timestamp']\n",
    "    test_start = train_data.iloc[test_idx[0]]['timestamp']\n",
    "    test_end = train_data.iloc[test_idx[-1]]['timestamp']\n",
    "    \n",
    "    split_info.append({\n",
    "        'Fold': f'Fold {i+1}',\n",
    "        'í›ˆë ¨ì‹œì‘': train_start.strftime('%Y-%m-%d'),\n",
    "        'í›ˆë ¨ì¢…ë£Œ': train_end.strftime('%Y-%m-%d'),\n",
    "        'ê²€ì¦ì‹œì‘': test_start.strftime('%Y-%m-%d'),\n",
    "        'ê²€ì¦ì¢…ë£Œ': test_end.strftime('%Y-%m-%d'),\n",
    "        'í›ˆë ¨í¬ê¸°': len(train_idx),\n",
    "        'ê²€ì¦í¬ê¸°': len(test_idx)\n",
    "    })\n",
    "\n",
    "split_df = pd.DataFrame(split_info)\n",
    "print(split_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\n3ï¸âƒ£ ì‹œê³„ì—´ ë¶„í•  vs ëœë¤ ë¶„í•  ì„±ëŠ¥ ë¹„êµ:\")\n",
    "\n",
    "# ê°„ë‹¨í•œ ëª¨ë¸ë¡œ ì„±ëŠ¥ ë¹„êµ\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# 1) ì‹œê³„ì—´ ë¶„í• ë¡œ í•™ìŠµ/í‰ê°€\n",
    "model_ts = LinearRegression()\n",
    "model_ts.fit(X_train_ts, y_train_ts)\n",
    "y_pred_ts = model_ts.predict(X_test_ts)\n",
    "\n",
    "mse_ts = mean_squared_error(y_test_ts, y_pred_ts)\n",
    "mae_ts = mean_absolute_error(y_test_ts, y_pred_ts)\n",
    "\n",
    "# 2) ëœë¤ ë¶„í• ë¡œ í•™ìŠµ/í‰ê°€\n",
    "X_train_random, X_test_random, y_train_random, y_test_random = train_test_split(\n",
    "    X_regression, y_regression, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "model_random = LinearRegression()\n",
    "model_random.fit(X_train_random, y_train_random)\n",
    "y_pred_random = model_random.predict(X_test_random)\n",
    "\n",
    "mse_random = mean_squared_error(y_test_random, y_pred_random)\n",
    "mae_random = mean_absolute_error(y_test_random, y_pred_random)\n",
    "\n",
    "print(\"ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼:\")\n",
    "performance_df = pd.DataFrame({\n",
    "    'ë¶„í• ë°©ë²•': ['ì‹œê³„ì—´ ë¶„í• ', 'ëœë¤ ë¶„í• '],\n",
    "    'MSE': [mse_ts, mse_random],\n",
    "    'MAE': [mae_ts, mae_random],\n",
    "    'ì„¤ëª…': [\n",
    "        'ì‹œê°„ ìˆœì„œ ìœ ì§€ (í˜„ì‹¤ì )',\n",
    "        'ëœë¤ ì…”í”Œ (ë¹„í˜„ì‹¤ì )'\n",
    "    ]\n",
    "})\n",
    "print(performance_df.round(4))\n",
    "\n",
    "print(f\"\\n4ï¸âƒ£ êµì°¨ê²€ì¦ ì„±ëŠ¥ í‰ê°€:\")\n",
    "\n",
    "# TimeSeriesSplitìœ¼ë¡œ êµì°¨ê²€ì¦\n",
    "ts_scores = cross_val_score(LinearRegression(), X_train_ts, y_train_ts, \n",
    "                           cv=tscv, scoring='neg_mean_squared_error')\n",
    "\n",
    "# ì¼ë°˜ êµì°¨ê²€ì¦ (ë¹„êµìš©)\n",
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "normal_scores = cross_val_score(LinearRegression(), X_train_ts, y_train_ts,\n",
    "                               cv=kfold, scoring='neg_mean_squared_error')\n",
    "\n",
    "print(\"ğŸ“Š êµì°¨ê²€ì¦ ì„±ëŠ¥ ë¹„êµ:\")\n",
    "cv_comparison = pd.DataFrame({\n",
    "    'TimeSeriesSplit': -ts_scores,\n",
    "    'KFold(shuffle=True)': -normal_scores\n",
    "})\n",
    "print(\"MSE ì ìˆ˜:\")\n",
    "print(cv_comparison.round(4))\n",
    "print(f\"\\nTimeSeriesSplit í‰ê·  MSE: {-ts_scores.mean():.4f} (Â±{ts_scores.std():.4f})\")\n",
    "print(f\"KFold í‰ê·  MSE: {-normal_scores.mean():.4f} (Â±{normal_scores.std():.4f})\")\n",
    "\n",
    "print(f\"\\n5ï¸âƒ£ ì‹œê³„ì—´ ë¶„í•  ì‹œê°í™”:\")\n",
    "\n",
    "# ì‹œê³„ì—´ ë¶„í•  ì‹œê°í™”\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 8))\n",
    "\n",
    "# ì „ì²´ ë°ì´í„° ì‹œê³„ì—´ í”Œë¡¯\n",
    "sample_data = sensor_data_sorted[::24]  # í•˜ë£¨ë§ˆë‹¤ ìƒ˜í”Œë§\n",
    "axes[0].plot(sample_data['timestamp'], sample_data['temperature'], alpha=0.7)\n",
    "axes[0].axvline(x=train_data['timestamp'].max(), color='red', linestyle='--', \n",
    "                label=f'ë¶„í• ì  ({train_data[\"timestamp\"].max().strftime(\"%Y-%m-%d\")})')\n",
    "axes[0].set_title('ì‹œê³„ì—´ ë°ì´í„° ë¶„í• ')\n",
    "axes[0].set_xlabel('ì‹œê°„')\n",
    "axes[0].set_ylabel('ì˜¨ë„ (Â°C)')\n",
    "axes[0].legend()\n",
    "\n",
    "# í›ˆë ¨/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ë¶„í¬ ë¹„êµ\n",
    "axes[1].hist(y_train_ts, bins=50, alpha=0.7, label='í›ˆë ¨ì„¸íŠ¸', color='blue')\n",
    "axes[1].hist(y_test_ts, bins=50, alpha=0.7, label='í…ŒìŠ¤íŠ¸ì„¸íŠ¸', color='red')\n",
    "axes[1].set_title('í›ˆë ¨/í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì˜¨ë„ ë¶„í¬')\n",
    "axes[1].set_xlabel('ì˜¨ë„ (Â°C)')\n",
    "axes[1].set_ylabel('ë¹ˆë„')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nâœ… Time Series Splitting ì™„ë£Œ!\")\n",
    "print(f\"ğŸ¯ í•µì‹¬: ì‹œê³„ì—´ ë°ì´í„°ì—ì„œëŠ” ì‹œê°„ ìˆœì„œë¥¼ ë°˜ë“œì‹œ ìœ ì§€í•´ì•¼ í•©ë‹ˆë‹¤!\")\n",
    "print(f\"âš ï¸  Data Leakage ë°©ì§€ê°€ ê°€ì¥ ì¤‘ìš”í•œ í¬ì¸íŠ¸ì…ë‹ˆë‹¤!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1429d64",
   "metadata": {},
   "source": [
    "## âœ… **Section 7: Validate Split Quality**\n",
    "ë°ì´í„° ë¶„í• ì˜ í’ˆì§ˆì„ ê²€ì¦í•˜ê³  ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e338460a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… ë°ì´í„° ë¶„í•  í’ˆì§ˆ ê²€ì¦\n",
    "print(\"âœ… ë°ì´í„° ë¶„í•  í’ˆì§ˆ ê²€ì¦\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n1ï¸âƒ£ ë¶„í•  í’ˆì§ˆ ì²´í¬ë¦¬ìŠ¤íŠ¸:\")\n",
    "\n",
    "def validate_split_quality(X_train, X_test, y_train, y_test, split_name):\n",
    "    \"\"\"ë°ì´í„° ë¶„í•  í’ˆì§ˆì„ ì¢…í•©ì ìœ¼ë¡œ ê²€ì¦í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "    \n",
    "    print(f\"\\nğŸ“Š {split_name} í’ˆì§ˆ ê²€ì¦:\")\n",
    "    \n",
    "    # 1. ë°ì´í„° í¬ê¸° ê²€ì¦\n",
    "    total_size = len(X_train) + len(X_test)\n",
    "    train_ratio = len(X_train) / total_size\n",
    "    test_ratio = len(X_test) / total_size\n",
    "    \n",
    "    print(f\"   ğŸ“ ë°ì´í„° í¬ê¸°:\")\n",
    "    print(f\"      â€¢ í›ˆë ¨ì„¸íŠ¸: {len(X_train):,}ê°œ ({train_ratio:.1%})\")\n",
    "    print(f\"      â€¢ í…ŒìŠ¤íŠ¸ì„¸íŠ¸: {len(X_test):,}ê°œ ({test_ratio:.1%})\")\n",
    "    \n",
    "    # 2. í”¼ì²˜ ë¶„í¬ ê²€ì¦\n",
    "    feature_similarity = []\n",
    "    for col in X_train.columns:\n",
    "        train_mean = X_train[col].mean()\n",
    "        test_mean = X_test[col].mean()\n",
    "        train_std = X_train[col].std()\n",
    "        test_std = X_test[col].std()\n",
    "        \n",
    "        mean_diff = abs(train_mean - test_mean)\n",
    "        std_diff = abs(train_std - test_std)\n",
    "        \n",
    "        feature_similarity.append({\n",
    "            'feature': col,\n",
    "            'train_mean': train_mean,\n",
    "            'test_mean': test_mean,\n",
    "            'mean_diff': mean_diff,\n",
    "            'train_std': train_std,\n",
    "            'test_std': test_std,\n",
    "            'std_diff': std_diff\n",
    "        })\n",
    "    \n",
    "    feature_df = pd.DataFrame(feature_similarity)\n",
    "    print(f\"   ğŸ“ˆ í”¼ì²˜ ë¶„í¬ ìœ ì‚¬ì„±:\")\n",
    "    print(feature_df[['feature', 'mean_diff', 'std_diff']].round(3).to_string(index=False))\n",
    "    \n",
    "    # 3. íƒ€ê²Ÿ ë¶„í¬ ê²€ì¦\n",
    "    train_target_mean = y_train.mean()\n",
    "    test_target_mean = y_test.mean()\n",
    "    train_target_std = y_train.std()\n",
    "    test_target_std = y_test.std()\n",
    "    \n",
    "    target_mean_diff = abs(train_target_mean - test_target_mean)\n",
    "    target_std_diff = abs(train_target_std - test_target_std)\n",
    "    \n",
    "    print(f\"   ğŸ¯ íƒ€ê²Ÿ ë¶„í¬:\")\n",
    "    print(f\"      â€¢ í›ˆë ¨ í‰ê· : {train_target_mean:.2f}, í‘œì¤€í¸ì°¨: {train_target_std:.2f}\")\n",
    "    print(f\"      â€¢ í…ŒìŠ¤íŠ¸ í‰ê· : {test_target_mean:.2f}, í‘œì¤€í¸ì°¨: {test_target_std:.2f}\")\n",
    "    print(f\"      â€¢ í‰ê·  ì°¨ì´: {target_mean_diff:.2f}, í‘œì¤€í¸ì°¨ ì°¨ì´: {target_std_diff:.2f}\")\n",
    "    \n",
    "    # 4. í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°\n",
    "    avg_mean_diff = feature_df['mean_diff'].mean()\n",
    "    avg_std_diff = feature_df['std_diff'].mean()\n",
    "    \n",
    "    # ì ìˆ˜ (ì°¨ì´ê°€ ì ì„ìˆ˜ë¡ ì¢‹ìŒ)\n",
    "    quality_score = 100 - (avg_mean_diff + avg_std_diff + target_mean_diff + target_std_diff)\n",
    "    \n",
    "    print(f\"   â­ í’ˆì§ˆ ì ìˆ˜: {quality_score:.1f}/100\")\n",
    "    \n",
    "    return quality_score\n",
    "\n",
    "# ë‹¤ì–‘í•œ ë¶„í•  ë°©ë²•ì˜ í’ˆì§ˆ ê²€ì¦\n",
    "quality_results = []\n",
    "\n",
    "# 1) ì¼ë°˜ ë¶„í• \n",
    "score1 = validate_split_quality(X_train_reg, X_test_reg, y_train_reg, y_test_reg, \n",
    "                                \"ì¼ë°˜ Train-Test Split\")\n",
    "quality_results.append((\"ì¼ë°˜ ë¶„í• \", score1))\n",
    "\n",
    "# 2) ì‹œê³„ì—´ ë¶„í• \n",
    "score2 = validate_split_quality(X_train_ts, X_test_ts, y_train_ts, y_test_ts, \n",
    "                                \"ì‹œê³„ì—´ ê¸°ë°˜ Split\")\n",
    "quality_results.append((\"ì‹œê³„ì—´ ë¶„í• \", score2))\n",
    "\n",
    "print(f\"\\n2ï¸âƒ£ ëª¨ë¸ ì„±ëŠ¥ìœ¼ë¡œ ë¶„í•  í’ˆì§ˆ ê²€ì¦:\")\n",
    "\n",
    "# ë‹¤ì–‘í•œ ëª¨ë¸ë¡œ ì„±ëŠ¥ ë¹„êµ\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "performance_results = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    # ì¼ë°˜ ë¶„í•  ì„±ëŠ¥\n",
    "    model.fit(X_train_reg, y_train_reg)\n",
    "    y_pred_normal = model.predict(X_test_reg)\n",
    "    r2_normal = r2_score(y_test_reg, y_pred_normal)\n",
    "    \n",
    "    # ì‹œê³„ì—´ ë¶„í•  ì„±ëŠ¥\n",
    "    model.fit(X_train_ts, y_train_ts)\n",
    "    y_pred_ts = model.predict(X_test_ts)\n",
    "    r2_ts = r2_score(y_test_ts, y_pred_ts)\n",
    "    \n",
    "    performance_results.append({\n",
    "        'ëª¨ë¸': model_name,\n",
    "        'ì¼ë°˜ë¶„í• _R2': r2_normal,\n",
    "        'ì‹œê³„ì—´ë¶„í• _R2': r2_ts,\n",
    "        'ì„±ëŠ¥ì°¨ì´': abs(r2_normal - r2_ts)\n",
    "    })\n",
    "\n",
    "performance_df = pd.DataFrame(performance_results)\n",
    "print(\"ğŸ“Š ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ:\")\n",
    "print(performance_df.round(4))\n",
    "\n",
    "print(f\"\\n3ï¸âƒ£ ë°ì´í„° ëˆ„ìˆ˜(Data Leakage) ê²€ì‚¬:\")\n",
    "\n",
    "# ì‹œê°„ ê¸°ë°˜ ë°ì´í„° ëˆ„ìˆ˜ ê²€ì‚¬\n",
    "def check_temporal_leakage(train_data, test_data):\n",
    "    \"\"\"ì‹œê°„ ê¸°ë°˜ ë°ì´í„° ëˆ„ìˆ˜ë¥¼ ê²€ì‚¬í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "    \n",
    "    train_max_time = train_data['timestamp'].max()\n",
    "    test_min_time = test_data['timestamp'].min()\n",
    "    \n",
    "    leakage_detected = test_min_time <= train_max_time\n",
    "    \n",
    "    print(f\"   ğŸ“… í›ˆë ¨ ë°ì´í„° ìµœëŒ€ ì‹œê°„: {train_max_time}\")\n",
    "    print(f\"   ğŸ“… í…ŒìŠ¤íŠ¸ ë°ì´í„° ìµœì†Œ ì‹œê°„: {test_min_time}\")\n",
    "    \n",
    "    if leakage_detected:\n",
    "        print(f\"   ğŸš¨ ë°ì´í„° ëˆ„ìˆ˜ ë°œê²¬! ë¯¸ë˜ ì •ë³´ê°€ ê³¼ê±°ë¡œ ìœ ì¶œë¨\")\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"   âœ… ë°ì´í„° ëˆ„ìˆ˜ ì—†ìŒ! ì‹œê°„ ìˆœì„œ ì •ìƒ ìœ ì§€\")\n",
    "        return True\n",
    "\n",
    "# ì‹œê³„ì—´ ë¶„í•  ë°ì´í„° ëˆ„ìˆ˜ ê²€ì‚¬\n",
    "print(\"ğŸ” ì‹œê³„ì—´ ë¶„í•  ë°ì´í„° ëˆ„ìˆ˜ ê²€ì‚¬:\")\n",
    "leakage_free = check_temporal_leakage(train_data, test_data)\n",
    "\n",
    "print(f\"\\n4ï¸âƒ£ ìµœì¢… ê¶Œì¥ì‚¬í•­:\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ ë¶„í•  ë°©ë²•ë³„ ìš”ì•½:\")\n",
    "summary_df = pd.DataFrame({\n",
    "    'ë¶„í• ë°©ë²•': ['ì¼ë°˜ ëœë¤ë¶„í• ', 'ì‹œê³„ì—´ ë¶„í• ', 'ê³„ì¸µí™” ë¶„í• '],\n",
    "    'ì ìš©ìƒí™©': [\n",
    "        'ì¼ë°˜ì ì¸ ML ë¬¸ì œ',\n",
    "        'ì‹œê³„ì—´/ì„¼ì„œ ë°ì´í„°',\n",
    "        'ë¶ˆê· í˜• ë¶„ë¥˜ ë¬¸ì œ'\n",
    "    ],\n",
    "    'ì¥ì ': [\n",
    "        'ê°„ë‹¨í•˜ê³  ë¹ ë¦„',\n",
    "        'í˜„ì‹¤ì ì´ê³  ì•ˆì „í•¨',\n",
    "        'í´ë˜ìŠ¤ ë¶„í¬ ìœ ì§€'\n",
    "    ],\n",
    "    'ì£¼ì˜ì‚¬í•­': [\n",
    "        'ë°ì´í„° íŠ¹ì„± ê³ ë ¤',\n",
    "        'ì‹œê°„ ìˆœì„œ ë°˜ë“œì‹œ ìœ ì§€',\n",
    "        'íšŒê·€ë¬¸ì œ ì ìš© ì œí•œ'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nğŸ¯ ì˜¨ìŠµë„ ì„¼ì„œ ë°ì´í„° ë¶„ì„ ê²°ë¡ :\")\n",
    "print(f\"   âœ… ì‹œê³„ì—´ ë°ì´í„°ì´ë¯€ë¡œ TimeSeriesSplit ì‚¬ìš© ê¶Œì¥\")\n",
    "print(f\"   âœ… ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ë¥¼ ìœ„í•œ ì‹œê°„ ìˆœì„œ ìœ ì§€ í•„ìˆ˜\")\n",
    "print(f\"   âœ… êµì°¨ê²€ì¦ì‹œ TimeSeriesSplit í™œìš©\")\n",
    "print(f\"   âœ… ëª¨ë¸ ì„±ëŠ¥ì€ í˜„ì‹¤ì  ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ í‰ê°€\")\n",
    "\n",
    "print(f\"\\nğŸš€ WebEx ê°•ì˜ í•µì‹¬ í¬ì¸íŠ¸ ì •ë¦¬:\")\n",
    "print(f\"   1ï¸âƒ£ train_test_splitì˜ ë‹¤ì–‘í•œ ë§¤ê°œë³€ìˆ˜ í™œìš©\")\n",
    "print(f\"   2ï¸âƒ£ stratify ë§¤ê°œë³€ìˆ˜ë¡œ í´ë˜ìŠ¤ ë¶„í¬ ìœ ì§€\")\n",
    "print(f\"   3ï¸âƒ£ ì‹œê³„ì—´ ë°ì´í„°ì—ì„œ TimeSeriesSplit í•„ìˆ˜ ì‚¬ìš©\")\n",
    "print(f\"   4ï¸âƒ£ ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ê°€ ìµœìš°ì„  ê³ ë ¤ì‚¬í•­\")\n",
    "print(f\"   5ï¸âƒ£ ë¶„í•  í’ˆì§ˆ ê²€ì¦ì„ í†µí•œ ì‹ ë¢°ì„± í™•ë³´\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"ğŸ‰ ì˜¨ìŠµë„ ê´€ì¸¡ ë°ì´í„° ë¶„ì„ ì™„ë£Œ!\")\n",
    "print(\"ğŸ“š WebEx ê°•ì˜ ë‚´ìš©ì„ ë”°ë¼ì¡ìœ¼ì…¨ìŠµë‹ˆë‹¤!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
